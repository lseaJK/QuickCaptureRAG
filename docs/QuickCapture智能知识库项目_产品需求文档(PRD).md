好的，作为产品经理，我们来梳理一下这个项目的需求。这是一个非常有价值和前瞻性的想法，将传统的知识管理（RAG）与即时、可视化的信息捕获相结合。

---

### **产品需求文档 (PRD) - "QuickCapture" 智能知识库项目**

版本： V1.0

日期： 2025年8月29日

项目负责人： (您的名字)

#### 1. 项目愿景与目标

1.1. 项目愿景 (Vision)

打造一个无缝、即时、多模态的个人知识管理工具。用户在浏览任何信息时，无论是文本、图表还是UI界面，都能通过一个简单的快捷键，将其瞬间转化为可搜索、可理解、可交互的智能知识资产，构建一个真正意义上的“第二大脑”。

1.2. 核心问题 (Problem Statement)

当前知识获取和整理存在以下痛点：

- **信息捕获效率低：** 复制粘贴会丢失格式和图片，手动截图、命名、保存、归类的流程繁琐，严重打断用户心流。

- **非结构化数据无法利用：** 网页中的图表、代码截图、UI设计、错误提示等富含信息的图片，无法被传统文本搜索工具索引和理解。

- **知识是孤立的：** 保存的零散信息之间缺乏关联，难以形成知识网络，需要时难以快速回忆和调用。



#### 2. 目标用户画像 (User Persona)

- **研究人员/学生：** 快速抓取论文中的图表、数据、公式和关键段落，方便日后查询和引用。

- **软件开发者/UI设计师：** 随手收集代码片段、API文档、命令行错误、优秀的UI设计或组件，构建个人灵感库和问题解决方案库。

- **内容创作者/产品经理：** 捕获竞品分析截图、用户评论、数据报告、设计灵感，用于素材积累和需求分析。

- **所有知识工作者：** 任何需要快速记录和回顾屏幕信息，以提高工作和学习效率的人。

#### 3. 核心功能点 (Functional Requirements) - 详细拆解

我们将产品功能拆分为三个核心模块：**信息捕获端**、**后台处理引擎**和**智能交互端**。

---

##### **模块一：信息捕获端 (Capture Client)**

这是一个轻量级的桌面客户端或后台代理程序。

- **F1.1: 全局快捷键触发**
  
  - **描述：** 在操作系统的任何界面（浏览器、IDE、PDF阅读器、视频等），用户按下 `Ctrl+K` (或其他可自定义的快捷键)，即可激活截图功能。
  
  - **细节要求：**
    
    - 快捷键必须是全局的，不能与其他常用软件的热键冲突。
    
    - 首次启动时应引导用户进行快捷键设置或授权。
    
    - 响应必须是瞬时的，不能有可感知的延迟。

- **F1.2: 灵活的截图模式**
  
  - **描述：** 激活后，鼠标指针变为十字形，允许用户通过拖拽选择屏幕上的任意矩形区域。
  
  - **细节要求：**
    
    - **区域截图：** 默认模式，最常用。
    
    - **窗口截图 (可选)：** 鼠标悬停时自动识别窗口并高亮，单击即可捕获整个窗口。
    
    - **全屏截图 (可选)：** 提供一个选项或次级快捷键（如 `Ctrl+Shift+K`）进行全屏捕获。

- **F1.3: 即时反馈与快速标记 (Instant Feedback & Quick Tagging) **
  
  - **描述：** 截图完成后，图片将自动保存，同时系统给予一个轻量级的、可交互的即时反馈，允许用户快速为该知识片段添加标签。
  
  - **细节要求：**
    
    - 保存过程必须在后台静默进行。
    
    - 完成后，屏幕角落出现一个**可交互的通知**。
    
    - **通知功能：**
      
      - **基本信息：** 显示“已保存到知识库”。
      
      - **标签输入框：** 通知内包含一个标签（Tag）输入框，用户可以立即开始输入。
      
      - **标签建议与补全：** 当用户输入时，系统应根据已有的标签库，进行模糊匹配和自动补全，方便用户选择。
      
      - **新增标签：** 用户可以输入一个全新的词汇并回车，系统将自动创建这个新标签。支持一次添加多个标签（以逗号或空格分隔）。
    
    - **无干扰设计：** 如果用户忽略此通知，它将在几秒后自动消失。截图依然会成功保存，只是没有附加标签。这确保了核心流程的“无感”和高效。

- **F1.4: 自动元数据捕获与智能命名 (Automatic Metadata Capture & Smart Naming) **
  
  - **描述：** 在保存截图的同时，系统应尽可能捕获上下文元数据，并在后台处理后，为文件生成一个有意义的、易于理解的文件名。
  
  - **细节要求：**
    
    - **元数据捕获：**
      
      - **时间戳：** 自动记录截图的精确日期和时间。
      
      - **来源应用：** 记录截图时前台活动的应用名称。
      
      - **来源URL：** 如果来源是浏览器，应尝试捕获当前标签页的URL和标题。
    
    - **智能命名流程：**
      
      - **步骤1 - 初始保存：** 截图第一时间以时间戳格式（如 `2025-08-29_19-45-10.png`）保存到“收件箱”目录，确保捕获的即时性。
      
      - **步骤2 - 后台重命名：** 当**模块二**的后台处理引擎完成对图片内容的理解后，会生成一个简短的核心主题（例如“Python装饰器示例”）。该服务随后会自动将对应的截图文件重命名。
      
      - **最终文件名格式：** `[AI生成的核心主题]_[时间戳].png`，例如 `Python装饰器示例_2025-08-29_19-45-10.png`。这使得用户仅通过浏览文件夹也能快速识别内容。

---

##### **模块二：后台处理引擎 (Processing Engine)**

这是项目的技术核心，基于 LlamaIndex 和 ChromaDB 构建。它是一个后台服务，持续监控“知识库收件箱”目录。

- **F2.1: 文件监控与任务队列**
  
  - **描述：** 引擎实时监控指定的文件夹，一旦有新截图文件出现，立即将其加入处理队列。
  
  - **细节要求：**
    
    - 使用稳健的文件系统监控机制。
    
    - 建立任务队列，确保即使连续快速截图，也能按顺序、无丢失地处理。
    
    - 对处理失败的文件进行记录和重试。

- **F2.2: 多模态内容识别 (Multi-modal Parsing) - [V1.1 更新]**
  
  - **描述：** 对队列中的每张截图进行深度解析，提取其包含的所有信息。
  
  - **细节要求：**
    
    - **图像转文本 (OCR)：**
      
      - **首选方案：** 为追求最高的文字识别准确率，优先考虑调用成熟的商业级多模态模型API，例如 **KIMI的视觉模型**，其在处理复杂排版、手写体和中英文混合场景下通常表现优异。
      
      - **备选/本地方案：** 对于注重隐私或希望离线使用的用户，可以提供集成本地的开源OCR引擎（如 PaddleOCR）的选项。
    
    - **图像转描述 (Image Captioning)：**
      
      - 利用多模态大模型 (如 LLaVA, BLIP) 为图片生成一段自然的、全面的描述。
      
      - **(新增关联功能)** 此处生成的描述/摘要，也将被用于**模块一（F1.4）**中的智能文件命名。

- **F2.3: 智能切分与索引 (Chunking & Indexing)**
  
  - **描述：** 将识别出的文本、图像描述和用户添加的标签进行处理，并与原始图片关联，构建索引。
  
  - **细节要求：**
    
    - **数据结构：** 在LlamaIndex中，每个截图可以被视为一个 `ImageDocument`，其元数据中应包含用户在捕获时添加的**标签（Tags）**。
    
    - **文本切分：** 将OCR提取出的长文本，按照语义或固定长度切分成更小的 `TextNode`。
    
    - **图像节点：** 将整个图片作为一个 `ImageNode`。
    
    - **关联性：** 保证切分出的 `TextNode` 和 `ImageNode` 之间存在明确的关联，都指向原始的截图文件和其完整的元数据（包括标签、URL等）。

- **F2.4: 向量化与存储 (Vectorization & Storage)**
  
  - **描述：** 使用多模态嵌入模型将文本块、图片信息和标签转化为向量，并存入 ChromaDB。
  
  - **细节要求：**
    
    - **向量生成：**
      
      - 文本块（OCR内容、图像描述、**标签**、元数据）通过模型生成文本向量。
      
      - 原始图片通过模型的图像编码器生成图像向量。
    
    - **数据库存储：**
      
      - payload/metadata 应包含：原始文本块、OCR内容、图像描述、指向**已重命名**的原图路径、时间戳、来源URL以及**用户标签**等所有上下文信息。

---

##### **模块三：智能交互端 (RAG Interface)**

这是一个简洁的查询界面，让用户可以与他们捕获的知识进行对话。

- **F3.1: 快速唤出界面**
  
  - **描述：** 用户通过另一个全局快捷键 (如 `Ctrl+Shift+Space`) 或点击系统托盘图标，可以快速唤出一个类似 Spotlight/Raycast 的悬浮搜索框。
  
  - **细节要求：**
    
    - 界面设计要极简，聚焦于输入和输出。
    
    - 唤出和隐藏动画要流畅、快速。

- **F3.2: 多模态自然语言查询**
  
  - **描述：** 用户在搜索框中输入任何自然语言问题。
  
  - **细节要求：**
    
    - 例如：“我之前截的那个关于Python装饰器的代码是什么样的？”
    
    - “找一下上周看到的那个季度销售额的图表。”
    
    - “上次那个部署失败的报错信息是什么？”

- **F3.3: 混合检索 (Hybrid Retrieval)**
  
  - **描述：** 系统将用户查询转化为向量，在 ChromaDB 中进行高效的相似度搜索，同时可以结合元数据（**特别是标签**）进行关键词过滤。
  
  - **细节要求：**
    
    - 查询中可以显式使用标签，例如：“找一下我标记为 #urgent 的那个报错截图”。
    
    - 查询 "Python装饰器代码" 不仅会匹配到OCR文本，也会通过向量相似度匹配到代码截图。

- **F3.4: 结果生成与呈现**
  
  - **描述：** 将检索到的最相关的文本块和图片信息（作为上下文）注入到一个大型语言模型 (LLM) 中，生成一个精准、简洁的回答，并附上原始证据。
  
  - **细节要求：**
    
    - **综合回答：** LLM根据上下文，直接用自然语言回答用户的问题。
    
    - **证据溯源 (核心)：**
      
      - 在生成的答案下方，必须以卡片或缩略图的形式，清晰地展示引用到的原始截图。
      
      - 用户点击缩略图可以预览大图，甚至提供“在文件管理器中显示”或“打开来源URL”的选项。
      
      - 这是建立用户信任和方便深入研究的关键。

- **F3.5: 对话式追问**
  
  - **描述：** 在一次查询后，用户可以基于当前答案继续提问，形成一个简短的对话。
  
  - **细节要求：**
    
    - 界面保留上下文，允许用户进行如“这张图里的X轴代表什么？”之类的追问。

#### 4. 非功能性需求 (Non-Functional Requirements)

- **性能：**
  
  - 截图响应时间 < 100ms。
  
  - 后台处理单个截图时间应在数秒内完成。
  
  - RAG查询响应时间 < 3秒。

- **资源占用：** 后台服务在空闲时应占用极低的CPU和内存。

- **隐私与安全：** 所有用户数据（截图、索引、元数据）默认完全在本地存储和处理，不上传任何云端。这是产品的核心信任基石。

- **易用性：** 零配置开箱即用，用户只需安装即可开始使用核心功能。

---

以上就是作为产品经理对这个项目的详细需求规划。这个规划明确了我们要为谁、解决什么问题、以及如何分阶段、分模块地实现这个激动人心的产品。下一步就是根据这个PRD进行技术选型评估和开发排期。
