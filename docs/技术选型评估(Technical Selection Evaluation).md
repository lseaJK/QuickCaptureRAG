好的，收到。现在我们已经有了一份清晰的产品需求文档 (PRD V1.1)，接下来的任务就是将其转化为可执行的技术方案和开发计划。

作为技术负责人/架构师，我将根据这份PRD进行技术选型评估和制定开发排期。

---

### **技术选型评估 (Technical Selection Evaluation)**

目标：选择能够满足PRD V1.1所有功能、性能和隐私要求的技术栈，同时确保开发效率和未来的可扩展性。核心组件 `LlamaIndex` 和 `ChromaDB` 已确定，我们将围绕它们构建整个系统。

#### **整体架构**

我们将采用 **客户端-服务端 (C/S)** 架构，即使所有组件都运行在本地。

- **客户端 (Client):** 一个轻量级的桌面应用程序，负责截图、通知、快捷键监听和展示查询界面。

- **服务端 (Server):** 一个后台Python服务，负责文件监控、AI处理、索引构建和提供RAG查询API。

---

#### **模块一：信息捕获端 (Capture Client)**

- **技术框架：**
  
  - **推荐：Tauri**
  
  - **理由：**
    
    1. **轻量级与高性能：** Tauri使用Rust作为后端，系统调用和性能密集型任务（如全局快捷键监听）效率远高于基于Node.js的Electron。应用打包后体积小，内存占用低，完美契合PRD中的“轻量级”要求。
    
    2. **跨平台：** 一套代码库即可支持Windows, macOS和Linux，降低开发和维护成本。
    
    3. **现代Web前端：** 可以使用React, Vue或Svelte等现代前端框架构建用户界面（截图选择框、交互式通知、RAG查询窗口），开发体验好，生态丰富。
    
    4. **安全性：** Rust的语言特性保证了更高的内存安全。

- **关键库/插件：**
  
  - **全局快捷键：** `tauri-plugin-global-shortcut`
  
  - **截图实现：** `screenshots-rs` (Rust库) 或通过Tauri调用系统原生API。
  
  - **桌面通知：** `@tauri-apps/api/notification` (支持可交互的通知)。
  
  - **UI框架：** **React** (配合 `Vite` 进行构建)。

---

#### **模块二：后台处理引擎 (Processing Engine)**

- **语言/框架：**
  
  - **推荐：Python 3.10+** (使用 FastAPI 或 Flask 提供API服务)
  
  - **理由：**
    
    1. **AI生态系统：** Python是AI/ML领域的首选语言，LlamaIndex、Hugging Face Transformers、PyTorch等所有核心库都基于Python。
    
    2. **快速开发：** FastAPI框架性能高，异步支持好，非常适合构建本地API服务，供Tauri客户端调用。

- **核心组件：**
  
  - **RAG框架 (已定)：** **LlamaIndex**
    
    - **职责：** 作为整个数据处理和查询流程的“胶水层”，连接数据源、解析器、嵌入模型、向量库和LLM。
  
  - **向量数据库 (已定)：** **ChromaDB**
    
    - **职责：** 本地化存储所有截图的向量和元数据。它的本地优先设计完全符合PRD的隐私要求。
  
  - **文件监控：** `watchdog` (Python库)，稳定可靠，用于监控截图“收件箱”。
  
  - **任务队列：**
    
    - **推荐：Celery** with **Redis** or **SQLite** as Broker。
    
    - **理由：** 当用户连续快速截图时，AI处理是耗时操作。引入任务队列可以实现异步处理，避免阻塞，并提供任务重试等高级功能，保证系统的稳定性。对于纯本地应用，使用SQLite作为Broker可以免去安装Redis的麻烦。

- **AI模型选型：**
  
  - **多模态模型 (用于OCR、图像描述、智能命名、向量化)：**
    
    - **推荐：Ollama + LLaVA 模型 (本地部署)**
    
    - **理由：**
      
      1. **一体化解决方案：** LLaVA (Large Language and Vision Assistant) 是强大的开源多模态模型，可以同时处理图像理解（生成描述用于智能命名）和OCR任务。
      
      2. **简化部署：** Ollama工具极大地简化了在本地运行和管理LLM/LMM的复杂度。只需一条命令即可下载并运行LLaVA。
      
      3. **隐私与成本：** 完全本地运行，无需API Key，没有额外费用，数据不出本地，满足最高隐私要求。
      
      4. **LlamaIndex集成：** LlamaIndex有现成的 `LlamaIndex-Multi-Modal-LLM-Ollama` 集成包。
  
  - **OCR 备选方案 (如PRD所述)：**
    
    - **KIMI API：** 如果用户追求极致的OCR准确性且不介意联网，可以在应用中内置一个选项，让用户填入自己的API Key来启用KIMI进行OCR。
  
  - **LLM (用于RAG最终答案生成)：**
    
    - **推荐：Ollama + Llama 3 8B / Mistral 7B**
    
    - **理由：** 这两款模型在性能和资源消耗之间取得了很好的平衡，在现代消费级硬件上运行速度尚可，足以支撑流畅的RAG问答体验。同样通过Ollama进行管理。

---

### **开发排期 (Development Schedule)**

假设投入1-2名全栈开发者，以下是一个分阶段的开发计划，总周期约为 **12周**。

| **阶段 (Phase)** | **名称**            | **周期 (周)** | **主要任务与产出**                                                                                                                                                         | **依赖关系** |
| -------------- | ----------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| **P0**         | **项目启动与技术验证**     | **1周**     | - 搭建开发环境 (Tauri, Python, Ollama)。<br>- 初始化项目仓库，配置CI/CD基础。<br>- **技术验证 (PoC):** 编写简单脚本，验证LlamaIndex+Ollama(LLaVA)+ChromaDB的核心数据处理流程是否通畅。                             | -        |
| **P1**         | **核心后台引擎开发**      | **3周**     | - 实现文件监控服务 (`watchdog`)。<br>- 搭建Celery任务队列处理截图。<br>- 集成LlamaIndex，实现对截图的多模态解析(OCR、描述)、向量化和存储到ChromaDB的全流程。<br>- 使用FastAPI封装后台处理逻辑，并提供初步的查询API。                      | P0       |
| **P2**         | **捕获客户端 (MVP)**   | **2周**     | - 使用Tauri构建基础桌面应用。<br>- 实现全局快捷键 (`Ctrl+K`) 监听。<br>- 实现核心截图功能（区域选择）。<br>- 实现截图后自动静默保存到指定目录（此时为时间戳命名）。                                                                | P0       |
| **P3**         | **RAG交互界面与端到端闭环** | **3周**     | - 在Tauri应用中开发RAG查询界面（Spotlight风格）。<br>- 实现快捷键 (`Ctrl+Shift+Space`) 唤出界面。<br>- 前端界面调用P1开发的后端API，实现查询、检索和展示。<br>- **完成核心功能闭环：** 用户可以截图，然后立即可以在查询界面中通过自然语言问答并看到结果和源截图。 | P1, P2   |
| **P4**         | **V1.1高级功能实现**    | **2周**     | - **智能命名：** 后端处理完成后，回调重命名截图文件。<br>- **快速标记：** 开发Tauri中的可交互通知，允许用户输入标签。<br>- 后端API支持接收标签，并将其存入ChromaDB元数据中。<br>- 检索逻辑支持按标签过滤。                                        | P3       |
| **P5**         | **测试、打包与发布准备**    | **1周**     | - 全功能测试，Bug修复。<br>- 性能优化（启动速度、查询延迟）。<br>- 为Windows和macOS制作安装包 (.msi, .dmg)。<br>- 编写用户文档和引导教程。                                                                       | P4       |
| **总计**         |                   | **~12周**   |                                                                                                                                                                     |          |

#### **排期说明：**

- **并行开发：** 在P1后台引擎开发的同时，可以开始P2客户端的基础框架搭建，但核心功能对接需要等P1的API完成。

- **风险：**
  
  - **本地模型性能：** 在中低端硬件上，本地多模态模型和LLM的运行速度可能不理想，需要进行充分测试和性能优化，或在应用中提供模型选择（例如，小型号模型或云端API选项）。
  
  - **跨平台兼容性：** 全局快捷键、窗口信息获取等功能在不同操作系统下可能有细微差异，需要预留时间进行测试和适配。

- **发布策略：** 完成P3后，就已经有了一个可用的核心产品 (MVP)。可以考虑发布一个内部测试版，收集反馈，然后在P4、P5完成后正式发布V1.1。
