# QuickCapture 项目任务分解 (V1.0 - 初始化)

## P0: 项目启动与技术验证 (Milestone 1)

- [ ] **任务 1: 环境搭建与脚手架**
    - [ ] 1.1: 初始化 Tauri + React 前端项目结构。
    - [ ] 1.2: 初始化 Python FastAPI 后端项目结构。
    - [ ] 1.3: 配置开发环境，包括 pre-commit hooks, linters, formatters。
    - [ ] 1.4: 建立项目代码仓库 (Git)，并完成首次提交。

- [ ] **任务 2: 核心技术 PoC (概念验证)**
    - [ ] 2.1: (后端) 验证 FastAPI 与 LlamaIndex 的基本集成。
    - [ ] 2.2: (后端) 验证 ChromaDB 的基本读写与向量检索。
    - [ ] 2.3: (后端) 验证 Ollama + LLaVA 模型服务可以成功调用并返回图像描述。
    - [ ] 2.4: (前端) 验证 Tauri 可以成功调用系统级 API（如全局快捷键、截图）。
    - [ ] 2.5: (前后端) 验证 Tauri 前端与 FastAPI 后端之间的基本通信 (IPC 或 HTTP)。

## P1: 核心后台处理引擎 (Milestone 2)

- [ ] **任务 3: 文件处理与任务队列**
    - [ ] 3.1: (后端) 设计并实现文件接收 API 接口。
    - [ ] 3.2: (后端) 集成 Celery 与 SQLite/Redis，建立异步任务处理队列。
    - [ ] 3.3: (后端) 实现一个文件监控模块 (使用 watchdog)，能监控指定目录的新增文件并将其加入任务队列。

- [ ] **任务 4: 多模态内容识别模块**
    - [ ] 4.1: (后端) 创建异步任务：对图像文件进行 OCR 文字识别。
    - [ ] 4.2: (后端) 创建异步任务：调用 LLaVA 模型生成图像的详细描述。
    - [ ] 4.3: (后端) 设计数据模型，用于存储原始内容、OCR 结果和图像描述。

- [ ] **任务 5: 内容索引与向量化**
    - [ ] 5.1: (后端) 创建异步任务：对识别出的文本内容进行智能切分 (Chunking)。
    - [ ] 5.2: (后端) 创建异步任务：将切分后的文本块进行向量化。
    - [ ] 5.3: (后端) 将文本块及其向量存储到 ChromaDB 中，并建立与原始文件的关联。

## P2: 捕获客户端 MVP (Milestone 3)

- [ ] **任务 6: 全局捕获功能**
    - [ ] 6.1: (前端) 实现全局快捷键注册与监听功能。
    - [ ] 6.2: (前端) 实现一个捕获窗口/界面，可通过快捷键唤起。
    - [ ] 6.3: (前端) 实现灵活的截图功能（全屏、选区）。

- [ ] **任务 7: 信息提交与管理**
    - [ ] 7.1: (前端) 在捕获窗口中，允许用户对截图进行预览。
    - [ ] 7.2: (前端) 实现一个简单的输入框，用于快速添加标记或标签。
    - [ ] 7.3: (前端) 实现文件（截图）和元数据（标签）到后端 API 的提交逻辑。
    - [ ] 7.4: (前端) 实现一个简单的历史记录查看界面。

## P3: RAG 智能交互端 (Milestone 4)

- [ ] **任务 8: 混合检索接口**
    - [ ] 8.1: (后端) 设计并实现一个搜索 API 接口，接收自然语言查询。
    - [ ] 8.2: (后端) 实现查询语句的向量化。
    - [ ] 8.3: (后端) 在 ChromaDB 中执行向量相似度检索。
    - [ ] 8.4: (后端) (可选) 实现关键词检索，并融合向量检索结果（混合检索）。

- [ ] **任务 9: RAG 交互界面**
    - [ ] 9.1: (前端) 创建一个主交互界面，包含搜索框和结果展示区。
    - [ ] 9.2: (前端) 实现调用后端搜索 API 的逻辑。
    - [ ] 9.3: (前端) 设计并实现搜索结果的展示，包括内容摘要和来源信息。
    - [ ] 9.4: (前端) 实现点击来源可追溯到原始截图或文件的功能。

- [ ] **任务 10: 结果生成与对话**
    - [ ] 10.1: (后端) 将检索到的上下文信息与用户问题组合成 Prompt。
    - [ ] 10.2: (后端) 调用一个文本生成模型（可通过 Ollama 配置）生成最终答案。
    - [ ] 10.3: (后端) (可选) 实现支持对话式追问的 API 逻辑。
    - [ ] 10.4: (前端) 在界面上展示由 LLM 生成的最终答案。
